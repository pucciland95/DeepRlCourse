{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import (\n",
    "    notebook_login,\n",
    ")  # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e22000",
   "metadata": {},
   "source": [
    "# Setting up StableBaseline 4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d55bf",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319f4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">1,015,808/1,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:23:20</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span> , <span style=\"color: #800000; text-decoration-color: #800000\">1,059 it/s</span> ]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m 100%\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1,015,808/1,000,000 \u001b[0m [ \u001b[33m0:23:20\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m1,059 it/s\u001b[0m ]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "mean_reward ppo=271.73 +/- 20.961501937338195\n",
      "mean_reward ppo=257.58 +/- 23.40161484122245\n",
      "Better PPO model trained! Saving...\n"
     ]
    }
   ],
   "source": [
    "import tensorboard\n",
    "env = make_vec_env(\"LunarLander-v3\", n_envs=16)\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO('MlpPolicy', \n",
    "            env=env, \n",
    "            tensorboard_log=\"../LunarLander-v3_tensorboard/\",\n",
    "            learning_rate=0.0005, \n",
    "            n_steps=2048, \n",
    "            batch_size=256, \n",
    "            n_epochs=10, \n",
    "            gamma=0.999, \n",
    "            gae_lambda=0.98, \n",
    "            clip_range=0.2, \n",
    "            clip_range_vf=None, \n",
    "            normalize_advantage=True, \n",
    "            ent_coef=0.01, \n",
    "            vf_coef=0.5, \n",
    "            max_grad_norm=0.5, \n",
    "            use_sde=False, \n",
    "            sde_sample_freq=-1,\n",
    "            rollout_buffer_class=None, \n",
    "            rollout_buffer_kwargs=None, \n",
    "            target_kl=None, \n",
    "            stats_window_size=100, \n",
    "            policy_kwargs=None, \n",
    "            verbose=0, \n",
    "            seed=None, \n",
    "            device='auto', \n",
    "            _init_setup_model=True)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(2e6), progress_bar=True)\n",
    "\n",
    "# Save Model to file only if reward is better\n",
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "ppo_new_model = model\n",
    "ppo_old_model = PPO.load(\"models/ppo_LunarLander-v3_model\", env=eval_env)\n",
    " \n",
    "mean_reward_new_ppo, std_new_reward_ppo = evaluate_policy(ppo_new_model, ppo_new_model.get_env(), n_eval_episodes=10, deterministic=True)\n",
    "mean_reward_old_ppo, std_reward_old_ppo = evaluate_policy(ppo_old_model, ppo_old_model.get_env(), n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "print(f\"mean_reward ppo={mean_reward_new_ppo:.2f} +/- {std_new_reward_ppo}\")\n",
    "print(f\"mean_reward ppo={mean_reward_old_ppo:.2f} +/- {std_reward_old_ppo}\")\n",
    "\n",
    "if mean_reward_new_ppo > mean_reward_old_ppo:\n",
    "    print(\"Better PPO model trained! Saving...\")\n",
    "    model.save(\"models/ppo_LunarLander-v3_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55a456",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "108beafb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m      3\u001b[39m env = make_vec_env(\u001b[33m\"\u001b[39m\u001b[33mLunarLander-v3\u001b[39m\u001b[33m\"\u001b[39m, n_envs=\u001b[32m16\u001b[39m)\n\u001b[32m      5\u001b[39m model = DQN(\u001b[33m\"\u001b[39m\u001b[33mMlpPolicy\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m      6\u001b[39m             env=env,\n\u001b[32m      7\u001b[39m             learning_rate=\u001b[32m0.001\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m             device=\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     30\u001b[39m             _init_setup_model=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m model.save(\u001b[33m\"\u001b[39m\u001b[33mmodels/dqn_LunarLander-v3_model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/DeepRlCourse/venv/lib/python3.12/site-packages/stable_baselines3/dqn/dqn.py:272\u001b[39m, in \u001b[36mDQN.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    264\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[32m    265\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    271\u001b[39m ) -> SelfDQN:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/DeepRlCourse/venv/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:335\u001b[39m, in \u001b[36mOffPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     rollout = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout.continue_training:\n\u001b[32m    346\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/DeepRlCourse/venv/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:567\u001b[39m, in \u001b[36mOffPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[39m\n\u001b[32m    564\u001b[39m actions, buffer_actions = \u001b[38;5;28mself\u001b[39m._sample_action(learning_starts, action_noise, env.num_envs)\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    570\u001b[39m num_collected_steps += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/DeepRlCourse/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/DeepRlCourse/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.buf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx] = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/DeepRlCourse/venv/lib/python3.12/site-packages/stable_baselines3/common/monitor.py:94\u001b[39m, in \u001b[36mMonitor.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.needs_reset:\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTried to step environment that needs reset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.rewards.append(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/DeepRlCourse/venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/DeepRlCourse/venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/DeepRlCourse/venv/lib/python3.12/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/DeepRlCourse/venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/DeepRlCourse/venv/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:621\u001b[39m, in \u001b[36mLunarLander.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    607\u001b[39m         p.ApplyLinearImpulse(\n\u001b[32m    608\u001b[39m             (\n\u001b[32m    609\u001b[39m                 ox * SIDE_ENGINE_POWER * s_power,\n\u001b[32m   (...)\u001b[39m\u001b[32m    613\u001b[39m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    614\u001b[39m         )\n\u001b[32m    615\u001b[39m     \u001b[38;5;28mself\u001b[39m.lander.ApplyLinearImpulse(\n\u001b[32m    616\u001b[39m         (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n\u001b[32m    617\u001b[39m         impulse_pos,\n\u001b[32m    618\u001b[39m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    619\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m.\u001b[49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mFPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m pos = \u001b[38;5;28mself\u001b[39m.lander.position\n\u001b[32m    624\u001b[39m vel = \u001b[38;5;28mself\u001b[39m.lander.linearVelocity\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/courses/DeepRlCourse/venv/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:64\u001b[39m, in \u001b[36mContactDetector.BeginContact\u001b[39m\u001b[34m(self, contact)\u001b[39m\n\u001b[32m     61\u001b[39m     contactListener.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mself\u001b[39m.env = env\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mBeginContact\u001b[39m(\u001b[38;5;28mself\u001b[39m, contact):\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     66\u001b[39m         \u001b[38;5;28mself\u001b[39m.env.lander == contact.fixtureA.body\n\u001b[32m     67\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.lander == contact.fixtureB.body\n\u001b[32m     68\u001b[39m     ):\n\u001b[32m     69\u001b[39m         \u001b[38;5;28mself\u001b[39m.env.game_over = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = make_vec_env(\"LunarLander-v3\", n_envs=16)\n",
    "\n",
    "model = DQN(\"MlpPolicy\", \n",
    "            env=env,\n",
    "            learning_rate=0.001, \n",
    "            buffer_size=5000000, \n",
    "            learning_starts=100, \n",
    "            batch_size=256, \n",
    "            tau=1.0, \n",
    "            gamma=0.999, \n",
    "            train_freq=50, \n",
    "            gradient_steps=10, \n",
    "            replay_buffer_class=None, \n",
    "            replay_buffer_kwargs=None, \n",
    "            optimize_memory_usage=False, \n",
    "            n_steps=1, \n",
    "            target_update_interval=10000, \n",
    "            exploration_fraction=0.2, \n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.05, \n",
    "            max_grad_norm=10, \n",
    "            stats_window_size=100, \n",
    "            tensorboard_log=\"../LunarLander-v3_tensorboard/\", \n",
    "            policy_kwargs=None, \n",
    "            verbose=0, \n",
    "            seed=None, \n",
    "            device='auto', \n",
    "            _init_setup_model=True)\n",
    "\n",
    "model.learn(total_timesteps=1e6)\n",
    "model.save(\"models/dqn_LunarLander-v3_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc56f0c",
   "metadata": {},
   "source": [
    "## PPO vs DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5056b6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "mean_reward ppo=258.52 +/- 16.865362307268374\n",
      "mean_reward dqn=-14.42 +/- 73.22439129315008\n"
     ]
    }
   ],
   "source": [
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "ppo_trained_model = PPO.load(\"models/ppo_LunarLander-v3_model\", env=eval_env)\n",
    "dqn_trained_model = DQN.load(\"models/dqn_LunarLander-v3_model\", env=eval_env)\n",
    "\n",
    "mean_reward_ppo, std_reward_ppo = evaluate_policy(ppo_trained_model, ppo_trained_model.get_env(), n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward ppo={mean_reward_ppo:.2f} +/- {std_reward_ppo}\")\n",
    "\n",
    "mean_reward_dqn, std_reward_dqn= evaluate_policy(dqn_trained_model, dqn_trained_model.get_env(), n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward dqn={mean_reward_dqn:.2f} +/- {std_reward_dqn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4406e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e7719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=-113.29 +/- 38.6918938208943\n"
     ]
    }
   ],
   "source": [
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "# trained_model = PPO.load(\"ppo_LunarLander-v3_model\", env=eval_env)\n",
    "trained_model = DQN.load(\"models/dqn_LunarLander-v3_model\", env=eval_env)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(trained_model, trained_model.get_env(), n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f056e7",
   "metadata": {},
   "source": [
    "# Graphical Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "# trained_model = PPO.load(\"models/ppo_LunarLander-v3_model\", env=eval_env)\n",
    "trained_model = DQN.load(\"models/dqn_LunarLander-v3_model\", env=eval_env)\n",
    "\n",
    "# Resetting the enironment\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(500):\n",
    "    # Take a random action \n",
    "    action, state = trained_model.predict(observation, )\n",
    "    \n",
    "    # Apply this action in the env\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode is finished\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002972bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ This function will save, evaluate, generate a video of your agent,\n",
      "create a model card and push everything to the hub. It might take up to 1min.\n",
      "This is a work in progress: if you encounter a bug, please open an issue.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicco/courses/DeepRlCourse/venv/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to /tmp/tmpu93ohzk_/-step-0-to-step-1000.mp4\n",
      "MoviePy - Building video /tmp/tmpu93ohzk_/-step-0-to-step-1000.mp4.\n",
      "MoviePy - Writing video /tmp/tmpu93ohzk_/-step-0-to-step-1000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready /tmp/tmpu93ohzk_/-step-0-to-step-1000.mp4\n",
      "\u001b[38;5;1m✘ 'DummyVecEnv' object has no attribute 'video_recorder'\u001b[0m\n",
      "\u001b[38;5;1m✘ We are unable to generate a replay of your agent, the package_to_hub\n",
      "process continues\u001b[0m\n",
      "\u001b[38;5;1m✘ Please open an issue at\n",
      "https://github.com/huggingface/huggingface_sb3/issues\u001b[0m\n",
      "\u001b[38;5;4mℹ Pushing repo Pucciland95/ppo-LunarLander-v3 to the Hugging Face\n",
      "Hub\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (4 / 4): 100%|██████████|  282kB /  282kB,  149kB/s  \n",
      "New Data Upload: 100%|██████████|  109kB /  109kB,  109kB/s  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Your model is pushed to the Hub. You can view your model here:\n",
      "https://huggingface.co/Pucciland95/ppo-LunarLander-v3/tree/main/\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Pucciland95/ppo-LunarLander-v3/commit/c7c678b9c912771347195671c06d64ccfbed56aa', commit_message='Uploaded PPO LunarLander-v3 trained agent', commit_description='', oid='c7c678b9c912771347195671c06d64ccfbed56aa', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Pucciland95/ppo-LunarLander-v3', endpoint='https://huggingface.co', repo_type='model', repo_id='Pucciland95/ppo-LunarLander-v3'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "repo_id = \"Pucciland95/ppo-LunarLander-v3\"\n",
    "env_id = \"LunarLander-v3\"\n",
    "\n",
    "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "\n",
    "model_architecture = \"PPO\"\n",
    "commit_message = \"Uploaded PPO LunarLander-v3 trained agent\"\n",
    "\n",
    "model = PPO.load(\"models/ppo_LunarLander-v3_model\", env=eval_env)\n",
    "\n",
    "package_to_hub(model=model, \n",
    "               model_name=\"ChopChopMotherFucker\",\n",
    "               model_architecture=model_architecture,\n",
    "               env_id=env_id,\n",
    "               eval_env=eval_env,\n",
    "               repo_id=repo_id,\n",
    "               commit_message=commit_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd7c70",
   "metadata": {},
   "source": [
    "# Cloning and Evaluating the Model you just pushed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8979e597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39 # 1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025\n",
      "- Python: 3.12.3\n",
      "- Stable-Baselines3: 2.7.0\n",
      "- PyTorch: 2.9.0+cu128\n",
      "- GPU Enabled: False\n",
      "- Numpy: 2.2.6\n",
      "- Cloudpickle: 3.1.1\n",
      "- Gymnasium: 1.2.1\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39 # 1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025\n",
      "- Python: 3.12.3\n",
      "- Stable-Baselines3: 2.7.0\n",
      "- PyTorch: 2.9.0+cu128\n",
      "- GPU Enabled: False\n",
      "- Numpy: 2.2.6\n",
      "- Cloudpickle: 3.1.1\n",
      "- Gymnasium: 1.2.1\n",
      "\n",
      "mean_reward=268.40 +/- 17.698211256933895\n"
     ]
    }
   ],
   "source": [
    "from huggingface_sb3 import load_from_hub\n",
    "\n",
    "repo_id = \"Pucciland95/ppo-LunarLander-v3\"\n",
    "filename = \"ChopChopMotherFucker.zip\"\n",
    "\n",
    "custom_objects = {\n",
    "            \"learning_rate\": 0.0,\n",
    "            \"lr_schedule\": lambda _: 0.0,\n",
    "            \"clip_range\": lambda _: 0.0,\n",
    "}\n",
    "\n",
    "checkpoint = load_from_hub(repo_id, filename)\n",
    "model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)\n",
    "\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v3\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
